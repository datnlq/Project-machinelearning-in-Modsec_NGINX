# -*- coding: utf-8 -*-
"""csic2010-prepare_for_keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tFvKlbhLKnlOHl7l33vzAJRFjyPONnYD
"""

# Ignore  the warnings
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')


# data visualisation and manipulation
import numpy as np
import pandas as pd

#nltk
import nltk

#preprocessing
from nltk.corpus import stopwords  #stopwords
from nltk import word_tokenize,sent_tokenize # tokenizing
from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet

# for part-of-speech tagging
from nltk import pos_tag

# for named entity recognition (NER)
from nltk import ne_chunk

# vectorizers for creating the document-term-matrix (DTM)
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer

# BeautifulSoup libraray
from bs4 import BeautifulSoup

import re # regex

#model_selection
from sklearn.model_selection import train_test_split,cross_validate
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

#evaluation
from sklearn.metrics import accuracy_score,roc_auc_score
from sklearn.metrics import classification_report
from mlxtend.plotting import plot_confusion_matrix

#preprocessing scikit
from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder

#classifiaction.
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC,SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB

#stop-words
nltk.download('stopwords')
stop_words=set(nltk.corpus.stopwords.words('english'))

#gensim w2v
#word2vec
from gensim.models import Word2Vec
import gensim
import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

#keras
import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
#from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense , Flatten ,Embedding,Input,LSTM
from keras.models import Model
from tensorflow.keras.preprocessing.text import text_to_word_sequence
from keras.layers import Bidirectional
from keras.layers import SpatialDropout1D
from keras.layers import Conv1D, GlobalMaxPool1D

#from keras.layers import LSTM

# Sử dụng cấu hình GPU tối ưu
# lstm_layer = LSTM(units=128, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', use_bias=False) CuDNNLSTM

#from google.colab import drive
#import os

#drive.mount('/content/drive')
#print(os.listdir('/content/drive/My Drive'))
#os.chdir('/content/drive/My Drive/Master_KMA/MachineLearning')
#!ls

normal_file = open("./datasets/baocao/normal-parsed.txt", encoding='cp437').readlines()

anomalous_file = open("./datasets/baocao/anomalous-parsed.txt", encoding='cp437').readlines()
df = {}
df['url'] = []
df['status'] = []
df['url'] = normal_file
df['status'] = [1]*len(normal_file)
for i in range(len(anomalous_file)):
    df['url'].append(anomalous_file[i])
    df['status'].append(0)

df = pd.DataFrame(data=df)
print(df['url'])

ano_df = df.loc[df.status==0, :]
nor_df = df.loc[df.status==1, :]
nor_df.head

df = pd.concat([ano_df, nor_df], ignore_index=True)
df.head()
# Shuffling rows
df = df.sample(frac=1).reset_index(drop=True)
df.head()

token_path = open("./token_baocao.txt", "r").readlines()
token_real = dict()
max_url_len = 556

c = 1
for i in token_path:
    token_real[i.strip('\n')] = c
    c = c + 1

def pad_texts_to_sq(url_test):
    lst_sq = []
    for u in url_test:
        sq = []
        u = u.replace('\n', '').split(' ')
        for i in u:
            ilow = i.lower()
            if (ilow in token_real):
                sq.append(token_real[ilow])

        # l = len(url_test)
        # while (l < max_url_len):
        #     sq.append(0)
        #     l = l + 1

        lst_sq.append(sq)
    return lst_sq

import nltk
nltk.download('punkt_tab')

# split word in url
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
words_url = []
sum = 0
for url in df['url']:
    words_url.append(url.split())

# for w in words_url[:5]:
#     print(w, '\n')

# find maximum length of url, then pad all to have this same length (required by keras embedding layer)

max_url_len = -1
for i, rev in enumerate(df['url']):
    tokens = rev.split()
    max_url_len = max(max_url_len, len(tokens))
max_url_len

# Integer encode
# tokenizer = Tokenizer(filters='\r\n')
# tokenizer.fit_on_texts(df['url'])
# vocab_size = len(tokenizer.word_index) + 1
# encd_url = tokenizer.texts_to_sequences(df['url'])
encd_url = list(df['url'])
print(max_url_len)

embed_dim = 300

pad_url = pad_sequences(pad_texts_to_sq(encd_url), maxlen=max_url_len, padding='post')
pad_url

# fo_token = open('./token_baocao.txt', "w")

# for i in tokenizer.word_index:
#     print(str(i))
#     fo_token.write(str(i)+'\n')

w2v_cbow = gensim.models.KeyedVectors.load_word2vec_format('./datasets/CSIC2010/cbow_w2v_baocao', encoding='cp437', binary=False)

w2v_cbow.key_to_index

# cbow
vocab_size = len(list(w2v_cbow.key_to_index))

embed_matrix = np.zeros(shape=(vocab_size +1, embed_dim))
for word, i in token_real.items():
    embed_vector = w2v_cbow.get_vector(word)
    if embed_vector is not None:
        embed_matrix[i] = embed_vector
embed_matrix.shape

vocab = list(w2v_cbow.key_to_index)
word_vec_dict = {}
for word in vocab:
    word_vec_dict[word] = w2v_cbow.get_vector(word)

len(word_vec_dict)

from keras import callbacks
earlystopping = callbacks.EarlyStopping(monitor ="val_loss",
                                        mode ="min", patience = 5,
                                        restore_best_weights = True)

Y = keras.utils.to_categorical(df['status'])
x_train, x_test, y_train, y_test = train_test_split(pad_url, Y, test_size=0.30, random_state=42)
epochs = 40
batch_size = 256

#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
inp = keras.Input(shape=(max_url_len,))

x = keras.layers.Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], trainable=True)(inp)
print(x.shape)
x = keras.layers.SpatialDropout1D(0.35)(x)
print(x.shape)

x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)
print(x.shape)
x = keras.layers.Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)
print(x.shape)

avg_pool = keras.layers.GlobalAveragePooling1D()(x)
max_pool = keras.layers.GlobalMaxPooling1D()(x)
x = keras.layers.Concatenate()([avg_pool, max_pool])
print(x.shape)

out = keras.layers.Dense(2, activation='softmax')(x)

model1 = keras.Model(inp, out)
#model.summary()
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# hist = model1.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])
# model1.save('./datasets/CSIC2010/model_lstm_cnn2d.keras')
# model.save('./datasets/CSIC2010/model_lstm_cnn2d.h5')

num_filters = 64
filter_sizes = [2,3,5]
inputs = keras.Input(shape=(max_url_len,), dtype='int32')
embedding = keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=embed_dim, input_length=max_url_len, weights=[embed_matrix])(inputs)

#embedding = keras.layers.Embedding(max_features, 300)(inputs)
reshape = keras.layers.Reshape((max_url_len,embed_dim,1))(embedding)

conv_0 = keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)
conv_1 = keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)
conv_2 = keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)

maxpool_0 = keras.layers.MaxPool2D(pool_size=(max_url_len - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)
maxpool_1 = keras.layers.MaxPool2D(pool_size=(max_url_len - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)
maxpool_2 = keras.layers.MaxPool2D(pool_size=(max_url_len - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)

concatenated_tensor = keras.layers.Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
flatten = keras.layers.Flatten()(concatenated_tensor)
dropout = keras.layers.Dropout(0.2)(flatten)
output = keras.layers.Dense(units=2, activation='softmax')(dropout)

# this creates a model that includes
model2 = keras.Model(inputs=inputs, outputs=output)

model2.summary()
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

#hist2 = model2.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

inp = keras.Input(shape=(max_url_len,))

x = keras.layers.Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], trainable=True)(inp)
x = keras.layers.SpatialDropout1D(0.35)(x)

x = keras.layers.LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)(x)
x = keras.layers.Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)

avg_pool = keras.layers.GlobalAveragePooling1D()(x)
max_pool = keras.layers.GlobalMaxPooling1D()(x)
x = keras.layers.Concatenate()([avg_pool, max_pool])

out = keras.layers.Dense(2, activation='softmax')(x)

model3 = keras.Model(inp, out)
model3.summary()

model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

#hist3 = model3.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):
    inp = keras.Input(shape = (max_url_len,))
    x = keras.layers.Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], trainable=True)(inp)
    x1 = keras.layers.SpatialDropout1D(dr)(x)

    x = keras.layers.Bidirectional(keras.layers.GRU(units, return_sequences = True))(x1)
    x = keras.layers.Conv1D(int(units/2), kernel_size = 2, padding = "valid", kernel_initializer = "he_uniform")(x)

    y = keras.layers.Bidirectional(keras.layers.LSTM(units, return_sequences = True))(x1)
    y = keras.layers.Conv1D(int(units/2), kernel_size = 2, padding = "valid", kernel_initializer = "he_uniform")(y)

    avg_pool1 = keras.layers.GlobalAveragePooling1D()(x)
    max_pool1 = keras.layers.GlobalMaxPooling1D()(x)

    avg_pool2 = keras.layers.GlobalAveragePooling1D()(y)
    max_pool2 = keras.layers.GlobalMaxPooling1D()(y)


    X = keras.layers.Concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])

    out = keras.layers.Dense(2, activation='sigmoid')(x)
    model = keras.Model(inp, out)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

    return model

model4 = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)
model4.summary()
###############################################################################

#hist4 = model4.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

model55 = keras.Sequential()
model55.add(keras.layers.Embedding(vocab_size + 1, embed_dim ,input_length=max_url_len))
model55.add(keras.layers.GlobalAveragePooling1D())
model55.add(keras.layers.Dense(16, activation='relu'))
model55.add(keras.layers.Dense(2, activation='sigmoid'))
model55.summary()

model55.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

hist55 = model55.fit(x_train,y_train, batch_size=64, epochs=40, validation_split=0.2) #mô hình đơn giản và hiệu quả với TensorFlow/Keras

model55.save('./models/model11.keras')
model55.save('./models/model11.h5')

from keras.layers import LSTM, GRU,SimpleRNN
model5 = Sequential()
model5.add(Embedding(vocab_size + 1, embed_dim ,input_length=max_url_len))
model5.add(SimpleRNN(100))
model5.add(Dense(2, activation='sigmoid'))
model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #RNN cơ bản (SimpleRNN)

hist5 = model5.fit(x_train,y_train, batch_size=64, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

model6 = Sequential()
model6.add(Embedding(vocab_size + 1,
                 embed_dim,
                 weights=[embed_matrix],
                 input_length=max_url_len,
                 trainable=False))

model6.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))
model6.add(Dense(2,activation='sigmoid'))
model6.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

# hist6 = model6.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

# GRU with glove embeddings and two dense layers
model7 = Sequential()
model7.add(Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], input_length=max_url_len, trainable=False))
model7.add(SpatialDropout1D(0.3))
model7.add(GRU(300))
model7.add(Dense(2, activation='sigmoid'))

model7.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

# hist7 = model7.fit(x_train,y_t rain, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

# A simple bidirectional LSTM with glove embeddings and one dense layer
model8 = Sequential()
model8.add(Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], input_length=max_url_len, trainable=False))
model8.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))

model8.add(Dense(2,activation='sigmoid'))
model8.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

hist8 = model8.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

def getModel():
    embedding_layer = Embedding(input_dim = vocab_size + 1,
                                output_dim = embed_dim,
                                weights=[embed_matrix],
                                input_length=max_url_len,
                                trainable=False)

    model = Sequential([
        embedding_layer,
        Bidirectional(LSTM(300, dropout=0.3, return_sequences=True)),
        Bidirectional(LSTM(300, dropout=0.3, return_sequences=True)),
        Conv1D(300, 5, activation='relu'),
        GlobalMaxPool1D(),
        Dense(16, activation='relu'),
        Dense(2, activation='sigmoid'),
    ],
    name="Sentiment_Model")
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model9 = getModel()
model9.summary()

# hist9 = model9.fit(x_train,y_train, batch_size=32, epochs=40, validation_split=0.2)#, callbacks =[earlystopping])

len(w2v_cbow.vectors)
# model9.save('./datasets/CSIC2010/model9.keras')
# model9.save('./datasets/CSIC2010/model9.h5')




####################################################################################################################################
urls_test = open("./dataset/CSIC2010/anomalousTrafficTest-Parsed.txt", "r").readlines()
urls_test = urls_test

encd_url_test = tokenizer.texts_to_sequences(urls_test)
# print(encd_url_test)
pad_url_test = pad_sequences(encd_url_test, maxlen=max_url_len, padding='post')
# print(pad_url_test)

pred = model.predict(pad_url_test)
print(pred)
acc_pred = (pred > 0.5).astype("int")

# t = 0
# for i in acc_pred:
#   if i[0] == 1:
#     t = t+1

# print(t/len(acc_pred))

model.save('./datasets/CSIC2010/model_final_97.keras')
model.save('./datasets/CSIC2010/model_final_97.h5')

from tensorflow.keras.models import load_model
tempmodel = load_model('./datasets/CSIC2010/model_final_95.keras')
tempmodel.summary()
#tf.keras.models.saved_model.save(model, './datasets/CSIC2010/300_dims')

word_index = tok.word_index
with open("./datasets/CSIC2010/word_index.txt", "w") as f:
  for w in word_index:
    f.write(w + " ")

inp = keras.Input(shape=(max_url_len,))

x = keras.layers.Embedding(vocab_size + 1, embed_dim, weights=[embed_matrix], trainable=True)(inp)

x = keras.layers.SpatialDropout1D(0.35)(x)

x = keras.layers.LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)(x)
x = keras.layers.Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)

avg_pool = keras.layers.GlobalAveragePooling1D()(x)
max_pool = keras.layers.GlobalMaxPooling1D()(x)
x = keras.layers.Concatenate()([avg_pool, max_pool])

out = keras.layers.Dense(2, activation='softmax')(x)

model2 = keras.Model(inp, out)
model2.summary()

model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# hist2 = model2.fit(x_train,y_train, batch_size=batch_size, epochs=40, validation_split=0.15)

#predict

#url_test = open("./datasets/TongHop/anor-Parsed.txt", encoding='cp437').readlines()
#url_test = open("./20_normalTrafficTest-parsed.txt", encoding='cp437').readlines()
url_test = ['/ PathString ', '/ PathString / PathString / PathString . gif ?']

encd_url_test = tokenizer.texts_to_sequences(url_test)
pad_url_test = pad_sequences(encd_url_test, maxlen = max_url_len, padding = 'post')
# print(pad_url_test[0])

pred = model2.predict(pad_url_test)
print(pred)
acc_pred = (pred > 0.5).astype('int')
print(acc_pred)

t = 0
for i in acc_pred:
    # 1 la normal, 0 la anomalous
    if (i[1] > i[0]):
        t=t+1

print(t/len(acc_pred))

len(pad_url_test[0])

n = len(x_test)
#n = 200
pred2 = model1.predict(x_test[:n])

label_test = (y_test[:n, 1] > 0.5).astype('int').tolist()
predicted = pred2[:n, 1].tolist()

print('test auc: ', roc_auc_score(label_test, predicted))

# model1.save("./models/model1.keras")
# model2.save("./models/model2.keras")
# model3.save("./models/model3.keras")
model5.save("./models/model5.keras")
# model6.save("./models/model6.keras")
# model7.save("./models/model7.keras")
# model8.save("./models/model8.keras")
# model9.save("./models/model9.keras")

# model1.save("./models/model1.h5")
# model2.save("./models/model2.h5")
# model3.save("./models/model3.h5")
model5.save("./models/model5.h5")
# model6.save("./models/model6.h5")
# model7.save("./models/model7.h5")
# model8.save("./models/model8.h5")
# model9.save("./models/model9.h5")




model_openai = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(max_url_len,)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(2, activation='softmax')
])
model_openai.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# hist_model_openai = model_openai.fit(x_train,y_train, batch_size=32, epochs=10, validation_split=0.2) 

model_openai1 = keras.Sequential([
    keras.layers.Embedding(vocab_size, embed_dim, input_length=max_url_len),
    keras.layers.GlobalAveragePooling1D(),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(2, activation='softmax')
])

# Compile the model
model_openai1.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

hist_model_openai1 = model_openai1.fit(x_train,y_train, batch_size=32, epochs=10, validation_split=0.2)

model_openai1.save('./models/model_openai.keras')
model_openai1.save('./models/model_openai.h5')

